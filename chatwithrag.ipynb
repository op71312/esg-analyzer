{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "553b6fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\university\\project-quant\\esg chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import CamembertTokenizer, AutoModel\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846290b",
   "metadata": {},
   "source": [
    "1. โหลด FAISS index และ chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26354080",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = faiss.read_index(\"chunk_faiss.index\")\n",
    "with open(\"chunk_texts.pkl\", \"rb\") as f:\n",
    "    chunk_texts: List[str] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f45fb",
   "metadata": {},
   "source": [
    "2. เตรียม WangchanBERTa และฟังก์ชัน encode_sentences_wangchanberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf85cb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertModel(\n",
       "  (embeddings): CamembertEmbeddings(\n",
       "    (word_embeddings): Embedding(25005, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): CamembertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x CamembertLayer(\n",
       "        (attention): CamembertAttention(\n",
       "          (self): CamembertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): CamembertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): CamembertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): CamembertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): CamembertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "tokenizer = CamembertTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2226add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(\n",
    "    query: str,\n",
    "    max_length: int = 128\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    แปลงคำถาม (หรือข่าว) เป็น embedding shape = (1, 768)\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        [query],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden = outputs.last_hidden_state  # (1, L, H)\n",
    "\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()  # (1, L, H)\n",
    "    masked_hidden = last_hidden * mask                                         # (1, L, H)\n",
    "\n",
    "    summed = torch.sum(masked_hidden, dim=1)             # (1, H)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)       # (1,)\n",
    "    mean_pooled = summed / counts.unsqueeze(-1)           # (1, H)\n",
    "\n",
    "    return mean_pooled.cpu().numpy().astype(\"float32\")    # shape = (1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b93576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences_wangchanberta(\n",
    "    sentences: List[str],\n",
    "    max_length: int = 128,\n",
    "    batch_size: int = 16\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    แปลงลิสต์ประโยค (List[str]) เป็น embedding (mean-pooling) ด้วย WangchanBERTa\n",
    "    คืน numpy array shape = (N, 768)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch_sents = sentences[i : i + batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch_sents,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = enc[\"input_ids\"].to(device)\n",
    "            attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden = outputs.last_hidden_state  # (B, L, 768)\n",
    "\n",
    "            mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()  # (B, L, 768)\n",
    "            masked_hidden = last_hidden * mask                                          # (B, L, 768)\n",
    "\n",
    "            summed = torch.sum(masked_hidden, dim=1)             # (B, 768)\n",
    "            counts = torch.clamp(mask.sum(dim=1), min=1e-9)       # (B, 768) เพราะ mask ถูกขยายมิติมา\n",
    "            # แต่ counts ในที่นี้แต่ละตำแหน่งจะเท่ากับจำนวน token จริง → mean pooling\n",
    "            mean_pooled = summed / counts                         # (B, 768)\n",
    "\n",
    "            all_embeddings.append(mean_pooled.cpu().numpy())\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)  # (N, 768)\n",
    "    return all_embeddings.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fc49a",
   "metadata": {},
   "source": [
    "3. encode_query: ใช้ encode_sentences_wangchanberta กับ list ที่มีข้อความเดียว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfdbe150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(\n",
    "    text: str,\n",
    "    max_length: int = 128\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    แปลงข้อความ (เช่น ข่าว) เป็น embedding shape = (1, 768)\n",
    "    โดยเรียกผ่าน encode_sentences_wangchanberta([text])\n",
    "    \"\"\"\n",
    "    embeddings = encode_sentences_wangchanberta(\n",
    "        sentences=[text],\n",
    "        max_length=max_length,\n",
    "        batch_size=1\n",
    "    )\n",
    "    # embeddings จะเป็น numpy array shape (1, 768)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fee01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences_wangchanberta(\n",
    "    sentences: List[str],\n",
    "    max_length: int = 128,\n",
    "    batch_size: int = 16\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    แปลงลิสต์ประโยค (List[str]) เป็น embedding (mean-pooling) ด้วย WangchanBERTa\n",
    "    คืน numpy array shape = (N, 768)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch_sents = sentences[i : i + batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch_sents,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = enc[\"input_ids\"].to(device)\n",
    "            attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden = outputs.last_hidden_state  # (B, L, 768)\n",
    "\n",
    "            mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()  # (B, L, 768)\n",
    "            masked_hidden = last_hidden * mask                                          # (B, L, 768)\n",
    "\n",
    "            summed = torch.sum(masked_hidden, dim=1)             # (B, 768)\n",
    "            counts = torch.clamp(mask.sum(dim=1), min=1e-9)       # (B, 768) เพราะ mask ถูกขยายมิติมา\n",
    "            # แต่ counts ในที่นี้แต่ละตำแหน่งจะเท่ากับจำนวน token จริง → mean pooling\n",
    "            mean_pooled = summed / counts                         # (B, 768)\n",
    "\n",
    "            all_embeddings.append(mean_pooled.cpu().numpy())\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)  # (N, 768)\n",
    "    return all_embeddings.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2b4d9",
   "metadata": {},
   "source": [
    "4. ฟังก์ชันดึง top-k chunks จาก FAISS (ตรวจสอบมิติให้เป็น (1,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12b5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k_chunks(\n",
    "    query_embedding: np.ndarray,\n",
    "    top_k: int = 3\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    รับ query_embedding (1,768) → คืน list [(chunk_text, distance), ...] top_k\n",
    "    ตรวจสอบมิติและ dtype ให้ถูกต้องก่อนเรียก faiss.search\n",
    "    \"\"\"\n",
    "    # เปลี่ยนให้เป็น 2 มิติ (n_queries × dim) ถ้า shape ผิด\n",
    "    # แต่ encode_query จะคืน (1, 768) จึงไม่มีปัญหา\n",
    "    if query_embedding.ndim == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "    elif query_embedding.ndim > 2:\n",
    "        query_embedding = query_embedding.reshape(query_embedding.shape[0], -1)\n",
    "\n",
    "    # แปลง dtype ให้เป็น float32 (FAISS ต้องการ)\n",
    "    if query_embedding.dtype != np.float32:\n",
    "        query_embedding = query_embedding.astype(\"float32\")\n",
    "\n",
    "    # ตรวจสอบดีบัก \n",
    "    #print(\">>> Debug: FAISS index dim =\", faiss_index.d)               # ควรเป็น 768\n",
    "    #print(\">>> Debug: query_embedding.shape =\", query_embedding.shape)  # ควรเป็น (1, 768)\n",
    "\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    results: List[Tuple[str, float]] = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        results.append((chunk_texts[idx], float(dist)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9547b",
   "metadata": {},
   "source": [
    "5. ฟังก์ชัน retrieve_context: รวบ context จาก top-k chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a2a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(\n",
    "    news: str,\n",
    "    top_k: int = 3,\n",
    "    max_chars: int = 1000\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    1. encode ข่าว → embedding shape (1, 768)\n",
    "    2. retrieve top_k chunks → คืน list of (chunk_text, dist)\n",
    "    3. รวมแต่ละ chunk (คั่นด้วย '---') จนความยาวรวมไม่เกิน max_chars\n",
    "    \"\"\"\n",
    "    q_emb = encode_query(news, max_length=128)           # (1, 768)\n",
    "    retrieved = retrieve_top_k_chunks(q_emb, top_k=top_k)\n",
    "\n",
    "    contexts: List[str] = []\n",
    "    total_len = 0\n",
    "    for chunk_text, dist in retrieved:\n",
    "        if total_len + len(chunk_text) > max_chars:\n",
    "            break\n",
    "        contexts.append(chunk_text)\n",
    "        total_len += len(chunk_text)\n",
    "\n",
    "    context_str = \"\\n---\\n\".join(contexts)\n",
    "    return context_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ae237",
   "metadata": {},
   "source": [
    "6. เตรียม Ollama (esg-analyzer) และ PromptTemplate ใหม่"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d5f664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20488\\1765551914.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"esg-analyzer\", temperature=0.0)\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"esg-analyzer\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8356e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "คุณเป็นผู้ช่วยวิเคราะห์ข่าวหุ้นเชิง ESG ระดับผู้เชี่ยวชาญ\n",
    "\n",
    "Context from documents:\n",
    "{context}\n",
    "\n",
    "News:\n",
    "{news}\n",
    "\n",
    "งานของคุณคือ\n",
    "1. สรุป sentiment ของข่าว (เลือกหนึ่งใน: บวก / กลาง / ลบ) พร้อมเหตุผลอย่างน้อย 2–3 ประโยค\n",
    "2. วิเคราะห์ว่า “ข่าวนี้จะส่งผลอย่างไรต่อ ESG Rating” ของบริษัทในข่าว \n",
    "   ให้ระบุว่า “ESG Rating จะ <สูงขึ้น/ลดลง/ไม่เปลี่ยน>” พร้อมอธิบายเหตุผลอย่างน้อย 2–3 ประโยค\n",
    "   โดยพิจารณาจากผลกระทบต่อสิ่งแวดล้อม สังคม และธรรมาภิบาล\n",
    "   หากข่าวไม่เกี่ยวข้องกับ ESG ให้ตอบว่า “ไม่เปลี่ยน” พร้อมเหตุผล\n",
    "   ลองดูว่า {context} สามารถนำมาช่วยวิเคราะห์ได้หรือไม่ หากได้ให้ใช้ข้อมูลนั้นในการวิเคราะห์ด้วย\n",
    "\n",
    "**โครงสร้างการตอบ (ตอบครบ 2 หัวข้อ):**\n",
    "\n",
    "Sentiment: <บวก/กลาง/ลบ>  \n",
    "เหตุผลสรุป (2–3 ประโยค):  \n",
    "- …  \n",
    "- …  \n",
    "\n",
    "Impact on ESG Rating: <สูงขึ้น/ลดลง/ไม่เปลี่ยน>  \n",
    "เหตุผลประกอบ (2–3 ประโยค):  \n",
    "- …  \n",
    "- …\n",
    "\n",
    "**ตัวอย่าง (อย่า copy ตรงนี้ ให้โมเดลตอบเฉพาะส่วนโครงสร้างด้านบน):**  \n",
    "News: ปตท. เตรียมขยายลงทุนสีเขียว เพิ่มสัดส่วนพลังงานหมุนเวียน  \n",
    "- Sentiment: บวก  \n",
    "  เหตุผลสรุป (2–3 ประโยค):  \n",
    "    - ข่าวนี้ชี้ให้เห็นว่าปตท. ให้ความสำคัญกับธุรกิจสีเขียวอย่างจริงจัง  \n",
    "    - การเพิ่มสัดส่วนพลังงานสะอาดช่วยลดการปล่อยก๊าซเรือนกระจก  \n",
    "    - ทั้งยังเป็นสัญญาณเชิงบวกต่อผู้ลงทุนที่คำนึง ESG  \n",
    "- Impact on ESG Rating: สูงขึ้น  \n",
    "  เหตุผลประกอบ (2–3 ประโยค):  \n",
    "    - การลงทุนในพลังงานหมุนเวียนจะช่วยยกระดับคะแนนดัชนี ESG ด้านสิ่งแวดล้อมอย่างชัดเจน  \n",
    "    - โครงการใหม่จะช่วยลดความเสี่ยงเรื่องกฎระเบียบสิ่งแวดล้อม  \n",
    "    - ทำให้ผู้ถือหุ้นและนักวิเคราะห์มองว่า ESG Profile ของปตท. ดีขึ้น  \n",
    "\n",
    "**ให้ทำเช่นนี้สำหรับข่าวต่อไปนี้ (ตอบครบตามโครงสร้างด้านบน):**  \n",
    "{news}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf88dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20488\\1641642314.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"context\", \"news\"], template=template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e69953",
   "metadata": {},
   "source": [
    "7. ทดลองใช้งาน RAG + Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1975bdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG + Ollama (esg-analyzer) พร้อมใช้งาน ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20488\\2621994233.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(context=context_str, news=news_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ผลลัพธ์ที่ได้ ===\n",
      "\n",
      "**Sentiment:** กลาง\n",
      "\n",
      "เหตุผลสรุป (2–3 ประโยค):  \n",
      "- ข่าวนี้เป็นข้อมูลเชิงบวกต่อธุรกิจพลังงานของปตท. แต่ไม่ได้กล่าวถึงผลกระทบด้านสิ่งแวดล้อมโดยตรง  \n",
      "- การลงทุนในโครงการโอมานอาจช่วยลดความเสี่ยงเรื่องกฎระเบียบสิ่งแวดล้อม แต่ยังไม่มีผลกระทบเชิงบวกต่อ ESG Rating ทันที  \n",
      "- ผู้ถือหุ้นและนักวิเคราะห์อาจมองว่าข่าวนี้เป็นปัจจัยบวกที่ช่วยเสริมภาพลักษณ์ของบริษัทในด้านธุรกิจพลังงาน\n",
      "\n",
      "**Impact on ESG Rating:** ไม่เปลี่ยน\n",
      "\n",
      "เหตุผลประกอบ (2–3 ประโยค):  \n",
      "- ข่าวไม่ได้กล่าวถึงการปรับเปลี่ยนพฤติกรรมหรือโครงการใหม่ที่มีผลกระทบเชิงบวกต่อสิ่งแวดล้อมอย่างชัดเจน  \n",
      "- การลงทุนในโครงการโอมานอาจเป็นปัจจัยบวกที่ช่วยเสริมภาพลักษณ์ของบริษัท แต่ยังไม่ได้กล่าวถึงการปรับเปลี่ยนพฤติกรรมหรือโครงการใหม่ที่มีผลกระทบเชิงบวกต่อ ESG Rating  \n",
      "- จึงไม่มีผลกระทบเชิงบวกหรือเชิงลบที่ชัดเจนต่อ ESG Rating ของปตท. ในระยะสั้น\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=== RAG + Ollama (esg-analyzer) พร้อมใช้งาน ===\")\n",
    "    news_text = \"ปตท.สผ. กระทรวงพลังงานและแร่ธาตุโอมาน และพันธมิตร ลงนามขยายอายุสัญญาโครงการโอมาน แปลง 53\"\n",
    "\n",
    "    # ดึง context จาก PDF (RAG)\n",
    "    context_str = retrieve_context(news_text, top_k=3, max_chars=1000)\n",
    "    # print(\"\\n=== Context from PDF ===\\n\", context_str)\n",
    "\n",
    "    # ส่ง context + news เข้า LLMChain แล้วแสดงผล\n",
    "    result = chain.run(context=context_str, news=news_text)\n",
    "    print(\"\\n=== ผลลัพธ์ที่ได้ ===\\n\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02114ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
